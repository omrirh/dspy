from datasets import load_dataset
from dspy.teleprompt.pez import BootstrapFewShotWithPEZ
from dspy.teleprompt.finetune import BootstrapFinetune
from transformers import AutoModelForSequenceClassification, AutoTokenizer
import torch

# Load the GLUE SST-2 dataset
dataset = load_dataset("glue", "sst2")
trainset = dataset['train']


def pez_metric(gold, prediction):  # , trace, prompt):
    """
    Evaluates the performance of the model given the optimized prompt and prediction.

    Parameters:
    - gold: The ground truth labels for the dataset.
    - prediction: The model's prediction.
    - trace: Trace information (from PEZ).
    - prompt: The optimized prompt generated by PEZ.

    Returns:
    - A score (e.g., accuracy, 1.0 for correct prediction, 0.0 for incorrect).
    """
    # Check if the model's prediction matches the true label
    is_correct = gold['label'] == prediction['label']

    # Optionally, we could also inspect `trace` and `prompt` to add more sophisticated evaluation
    # For example, ensuring the prompt structure is maintained, or the trace shows proper usage
    # of the prompt in generating the prediction.

    return 1.0 if is_correct else 0.0


# Load teacher model (RoBERTa-large for prompt optimization and finetuning)
teacher_model_name = "roberta-large"
tokenizer = AutoTokenizer.from_pretrained(teacher_model_name)
teacher_model = AutoModelForSequenceClassification.from_pretrained(teacher_model_name, num_labels=2)

# Create student model for finetuning
finetune_student = BootstrapFinetune()  # Make sure it is appropriately initialized

# Initialize the PEZ-based few-shot optimizer
fewshot_optimizer = BootstrapFewShotWithPEZ(
    metric=pez_metric,
    max_bootstrapped_demos=4,  # Number of bootstrapped examples
    max_labeled_demos=16,  # Maximum number of labeled demos
    num_candidate_programs=8,  # Number of candidate programs to test
    prompt_len=5,  # Number of tokens in the prompt
    opt_iters=500,  # Number of optimization iterations
    lr=5e-5,  # Learning rate for optimization
    weight_decay=1e-4,  # Weight decay for optimization
    print_step=50,  # Print optimization status every 50 steps
    loss_weight=1.0  # Loss weight during optimization
)

# Compile the student model with few-shot optimization via PEZ
compiled_program = fewshot_optimizer.compile(
    student=finetune_student,
    teacher=teacher_model,  # Teacher model (e.g., RoBERTa-large)
    trainset=trainset  # GLUE SST-2 dataset
)

# Now `compiled_program` contains the student model with optimized few-shot prompts
print("Few-shot optimization and finetuning completed.")
